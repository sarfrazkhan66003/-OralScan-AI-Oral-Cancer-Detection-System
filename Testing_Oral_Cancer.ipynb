{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11444,"status":"ok","timestamp":1747900848562,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"a3MLwwxINUIQ"},"outputs":[],"source":["import os\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import accuracy_score\n","import argparse\n","\n","def test_model_accuracy(model_path, dataset_path, image_size=(224, 224), batch_size=32):\n","    # Load model\n","    model = load_model(model_path)\n","\n","    # Prepare data\n","    datagen = ImageDataGenerator(rescale=1./255)\n","\n","    test_generator = datagen.flow_from_directory(\n","        dataset_path,\n","        target_size=image_size,\n","        batch_size=batch_size,\n","        class_mode='binary',\n","        shuffle=False\n","    )\n","\n","    # Predict\n","    predictions = model.predict(test_generator)\n","    predicted_classes = (predictions \u003e 0.5).astype(\"int32\").flatten()\n","    true_classes = test_generator.classes\n","\n","    # Accuracy\n","    accuracy = accuracy_score(true_classes, predicted_classes)\n","    print(f\"\\n‚úÖ Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23460,"status":"ok","timestamp":1747900877127,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"FqGkUp8m2edl","outputId":"c69a05e7-5174-4cc1-9879-97f66c1ba60d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":731,"status":"ok","timestamp":1747900882071,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"Q4ggK0mF2euM","outputId":"45bd4a3a-7111-46d9-9361-aad3b5ccf58a"},"outputs":[{"name":"stdout","output_type":"stream","text":["üîç Folders inside dataset:\n","['Cancer', 'Non_cancer', 'Cnace_Size', 'Non_Cancer_Size']\n"]}],"source":["import os\n","\n","dataset_path = \"/content/drive/MyDrive/Data_set\"\n","\n","# Check subfolders inside train directory\n","print(\"üîç Folders inside dataset:\")\n","print(os.listdir(dataset_path))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1747900884785,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"Tc_f3fug3dsz","outputId":"ec43081c-8095-482c-ffaa-3d00e0693cb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Dataset cleaned!\n"]}],"source":["import shutil\n","\n","unwanted_folders = [\".ipynb_checkpoints\", \"other\"]\n","for folder in unwanted_folders:\n","    folder_path = os.path.join(dataset_path, folder)\n","    if os.path.exists(folder_path):\n","        shutil.rmtree(folder_path)\n","        print(f\"üóë Deleted: {folder}\")\n","\n","print(\"‚úÖ Dataset cleaned!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"1yw_7ubv4t6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["üîç Loading model from: /content/drive/MyDrive/one.h5\n","üìê Detected input size: (224, 224)\n","Found 7865 images belonging to 4 classes.\n","üß™ Testing on 7865 images...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m246/246\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2102s\u001b[0m 9s/step\n","\n","‚úÖ Model Accuracy: 22.12%\n"]}],"source":["import os\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","\n","def test_model_accuracy(model_path, dataset_path, batch_size=32):\n","    try:\n","        print(f\"üîç Loading model from: {model_path}\")\n","        model = load_model(model_path, compile=False)\n","\n","        # Detect model input image size\n","        image_size = model.input_shape[1:3]\n","        print(f\"üìê Detected input size: {image_size}\")\n","\n","        # Prepare test data generator\n","        datagen = ImageDataGenerator(rescale=1./255)\n","        test_generator = datagen.flow_from_directory(\n","            dataset_path,\n","            target_size=image_size,\n","            batch_size=batch_size,\n","            class_mode='binary',\n","            shuffle=False\n","        )\n","\n","        print(f\"üß™ Testing on {len(test_generator.filenames)} images...\")\n","\n","        # Predict\n","        predictions = model.predict(test_generator, verbose=1)\n","        predicted_classes = (predictions \u003e 0.5).astype(\"int32\").flatten()\n","        true_classes = test_generator.classes\n","\n","        # Calculate accuracy\n","        accuracy = accuracy_score(true_classes, predicted_classes)\n","        print(f\"\\n‚úÖ Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","    except Exception as e:\n","        print(\"üö® An error occurred while testing the model:\")\n","        print(str(e))\n","\n","\n","# EXAMPLE USAGE (you can change these paths)\n","if __name__ == \"__main__\":\n","    model_path = \"/content/drive/MyDrive/one.h5\"\n","    dataset_path = \"/content/drive/MyDrive/Data_set\"\n","\n","    test_model_accuracy(model_path, dataset_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsYZujCA-ksk"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    model_path = \"/content/drive/MyDrive/small_data.h5\"\n","    dataset_path = \"/content/drive/MyDrive/Data_set\"\n","\n","    test_model_accuracy(model_path, dataset_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzVAa-Q0-5BS"},"outputs":[],"source":["# import cv2\n","# import numpy as np\n","# import tensorflow as tf\n","# from tensorflow.keras.models import load_model\n","# import matplotlib\n","# matplotlib.use('Agg')  # prevent GUI backend issues\n","# import os\n","\n","# def generate_visual_prediction(model_path, image_path, output_path='xai_output.png'):\n","#     # Load model\n","#     print(f\"üîç Loading model from: {model_path}\")\n","#     model = load_model(model_path, compile=False)\n","#     image_size = model.input_shape[1:3]\n","\n","#     # Load image\n","#     img = cv2.imread(image_path)\n","#     original_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","#     resized_img = cv2.resize(original_img, image_size)\n","#     input_array = np.expand_dims(resized_img / 255.0, axis=0)\n","\n","#     # Predict\n","#     pred = model.predict(input_array)[0][0]\n","#     label = \"NON_CANCER\" if pred \u003e 0.5 else \"CANCER\"\n","#     confidence = float(pred) if label == \"CANCER\" else 1 - float(pred)\n","#     confidence_percent = confidence * 100\n","\n","#     # üß† Find last Conv2D layer\n","#     last_conv_layer = None\n","#     for layer in reversed(model.layers):\n","#         if isinstance(layer, tf.keras.layers.Conv2D):\n","#             last_conv_layer = layer.name\n","#             break\n","#     if last_conv_layer is None:\n","#         raise ValueError(\"‚ùå No Conv2D layer found in model.\")\n","\n","#     # Grad-CAM\n","#     grad_model = tf.keras.models.Model(\n","#         [model.inputs],\n","#         [model.get_layer(last_conv_layer).output, model.output]\n","#     )\n","\n","#     with tf.GradientTape() as tape:\n","#         conv_outputs, predictions = grad_model(input_array)\n","#         loss = predictions[:, 0]\n","\n","#     grads = tape.gradient(loss, conv_outputs)\n","#     if grads is None:\n","#         raise ValueError(\"‚ùå Gradient is None ‚Äî unable to compute Grad-CAM.\")\n","\n","#     pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","#     conv_outputs = conv_outputs[0]\n","#     heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n","#     heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","#     heatmap = heatmap.numpy()\n","\n","#     # Prepare heatmap\n","#     heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n","#     heatmap = np.uint8(255 * heatmap)\n","#     heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n","\n","#     # Overlay heatmap\n","#     overlayed_img = cv2.addWeighted(original_img, 0.6, heatmap_color, 0.4, 0)\n","\n","#     # üìù Add prediction text with confidence\n","#     font = cv2.FONT_HERSHEY_SIMPLEX\n","#     text = f\"PREDICTION: {label} ({confidence_percent:.2f}%)\"\n","#     cv2.putText(overlayed_img, text, (10, 30), font, 1, (255, 255, 255), 2)\n","\n","#     # üñºÔ∏è Combine original and heatmap\n","#     combined = np.hstack((original_img, overlayed_img))\n","#     combined_bgr = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)\n","#     cv2.imwrite(output_path, combined_bgr)\n","\n","#     print(f\"‚úÖ Visual output saved: {output_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2006,"status":"ok","timestamp":1745131260073,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"UTDvDIv4Ae60","outputId":"c35919d0-2d44-4e0a-eafd-71f0a7c1b708"},"outputs":[{"name":"stdout","output_type":"stream","text":["üîç Loading model from: /content/drive/MyDrive/one.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n","‚úÖ Enhanced Grad-CAM saved to xai_result.png\n"]}],"source":["# generate_visual_prediction(\n","#     model_path=\"/content/drive/MyDrive/one.h5\",\n","#     image_path=\"/content/sample_data/images.jpeg\",\n","#     output_path=\"xai_result.png\"\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5204,"status":"ok","timestamp":1745131268512,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"6IlH1xJdEXF2","outputId":"eef42cb4-45a0-4e8f-bb56-d26fedfa625f"},"outputs":[{"name":"stdout","output_type":"stream","text":["üîç Loading model from: /content/drive/MyDrive/small_data.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n","‚úÖ Enhanced Grad-CAM saved to xai_1result.png\n"]}],"source":["# generate_visual_prediction(\n","#     model_path=\"/content/drive/MyDrive/small_data.h5\",\n","#     image_path=\"/content/sample_data/images.jpeg\",\n","#     output_path=\"xai_1result.png\"\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":427,"status":"ok","timestamp":1745120836956,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"dwtGtziWQ91o","outputId":"3c91ba62-ce5c-4bf4-a38d-2c20ddf3e15a"},"outputs":[{"name":"stdout","output_type":"stream","text":["üìä Cancer images: 1727\n","üìä Non-Cancer images: 1739\n","‚úÖ Dataset has been balanced.\n"]}],"source":["import os\n","import random\n","\n","def balance_dataset(parent_folder_path):\n","    cancer_dir = os.path.join(parent_folder_path, 'Cancer')\n","    non_cancer_dir = os.path.join(parent_folder_path, 'Non_cancer')\n","\n","    # Check if both folders exist\n","    if not os.path.exists(cancer_dir) or not os.path.exists(non_cancer_dir):\n","        print(\"‚ùå 'cancer' or 'non_cancer' folder not found in given path.\")\n","        return\n","\n","    # List images\n","    cancer_images = [f for f in os.listdir(cancer_dir) if os.path.isfile(os.path.join(cancer_dir, f))]\n","    non_cancer_images = [f for f in os.listdir(non_cancer_dir) if os.path.isfile(os.path.join(non_cancer_dir, f))]\n","\n","    len_cancer = len(cancer_images)\n","    len_non_cancer = len(non_cancer_images)\n","\n","    print(f\"üìä Cancer images: {len_cancer}\")\n","    print(f\"üìä Non-Cancer images: {len_non_cancer}\")\n","\n","    if len_cancer == len_non_cancer:\n","        print(\"‚úÖ Dataset is already balanced.\")\n","        return\n","\n","    # Remove extra images\n","    if len_cancer \u003e len_non_cancer:\n","        to_delete = random.sample(cancer_images, len_cancer - len_non_cancer)\n","        for img in to_delete:\n","            os.remove(os.path.join(cancer_dir, img))\n","    else:\n","        to_delete = random.sample(non_cancer_images, len_non_cancer - len_cancer)\n","        for img in to_delete:\n","            os.remove(os.path.join(non_cancer_dir, img))\n","\n","    print(\"‚úÖ Dataset has been balanced.\")\n","\n","# ‚¨áÔ∏è Bas yahan apna folder ka path daalo\n","balance_dataset(\"/content/drive/MyDrive/Data_set\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUkAhbkiRB-1"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","\n","def generate_visual_prediction(model_path, image_path, output_path='xai_result.png'):\n","    model = load_model(model_path, compile=False)\n","    img_size = model.input_shape[1:3]\n","\n","    # Load and preprocess image\n","    img = cv2.imread(image_path)\n","    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    resized = cv2.resize(rgb_img, img_size)\n","    input_tensor = np.expand_dims(resized / 255.0, axis=0)\n","\n","    # Predict\n","    pred = model.predict(input_tensor)[0][0]\n","    class_idx = int(pred \u003c= 0.5)\n","    confidence = pred if class_idx == 0 else 1 - pred\n","    label = \"NON-CANCER\" if class_idx == 0 else \"CANCER\"\n","\n","    # Grad-CAM setup\n","    conv_layer = None\n","    for l in reversed(model.layers):\n","        if isinstance(l, tf.keras.layers.Conv2D):\n","            conv_layer = l.name\n","            break\n","    if not conv_layer:\n","        raise ValueError(\"Conv2D layer not found.\")\n","\n","    grad_model = tf.keras.models.Model(\n","        [model.inputs],\n","        [model.get_layer(conv_layer).output, model.output]\n","    )\n","\n","    with tf.GradientTape() as tape:\n","        conv_output, predictions = grad_model(input_tensor)\n","        loss = predictions[:, 0]\n","\n","    grads = tape.gradient(loss, conv_output)[0]\n","    conv_output = conv_output[0]\n","    weights = tf.reduce_mean(grads, axis=(0, 1))\n","    cam = np.zeros(conv_output.shape[:2], dtype=np.float32)\n","\n","    for i, w in enumerate(weights):\n","        cam += w * conv_output[:, :, i]\n","\n","    cam = np.maximum(cam, 0)\n","    cam = cv2.resize(cam, (rgb_img.shape[1], rgb_img.shape[0]))\n","    cam = cam / (np.max(cam) + 1e-8)\n","\n","    # Heatmap\n","    heatmap = np.uint8(255 * cam)\n","    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n","    overlay = cv2.addWeighted(rgb_img, 0.6, heatmap_color, 0.4, 0)\n","\n","    # Add label and confidence - placed in bottom-right corner\n","    font = cv2.FONT_HERSHEY_SIMPLEX\n","    font_scale = 0.6\n","    thickness = 2\n","    label_text = f\"{label} ({confidence*100:.2f}%)\"\n","    text_size = cv2.getTextSize(label_text, font, font_scale, thickness)[0]\n","    x = overlay.shape[1] - text_size[0] - 20\n","    y = overlay.shape[0] - 20\n","    cv2.putText(overlay, label_text, (x, y), font, font_scale, (255, 255, 255), thickness)\n","\n","    # Combine original and heatmap overlay\n","    combined = np.hstack((rgb_img, overlay))\n","    result = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)\n","    cv2.imwrite(output_path, result)\n","    print(f\"[‚úÖ] Output image saved to: {output_path}\")\n","\n","# # Example usage\n","# generate_visual_prediction(\n","#     model_path=\"/content/drive/MyDrive/one.h5\",\n","#     image_path=\"/content/sample_data/images (2).jpeg\",\n","#     output_path=\"xai_result.png\"\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13230,"status":"ok","timestamp":1746451337998,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"sUZFxJwF8-Oq","outputId":"f0f3c173-eb5a-45ff-d16f-adf7736926db"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: [['input_layer_3']]\n","Received: inputs=Tensor(shape=(1, 224, 224, 3))\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["[‚úÖ] Output image saved to: xai_result.png\n"]}],"source":["generate_visual_prediction(\n","    model_path=\"/content/drive/MyDrive/one.h5\",\n","    image_path=\"/content/sample_data/aug_1_C008.jpeg\",\n","    output_path=\"xai_result.png\"\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5819,"status":"ok","timestamp":1746451343809,"user":{"displayName":"Sarfraz Khan","userId":"17874534209102713865"},"user_tz":-330},"id":"-ebMRmLr9VSy","outputId":"4223595b-183d-4c4e-b494-afb7428026d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 761ms/step\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: [['input_layer']]\n","Received: inputs=Tensor(shape=(1, 224, 224, 3))\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["[‚úÖ] Output image saved to: xai_1result.png\n"]}],"source":["generate_visual_prediction(\n","    model_path=\"/content/drive/MyDrive/small_data.h5\",\n","    image_path=\"/content/sample_data/aug_1_C008.jpeg\",\n","    output_path=\"xai_1result.png\"\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOQXMwq1D-y9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}